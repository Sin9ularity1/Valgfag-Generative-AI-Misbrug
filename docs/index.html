<!DOCTYPE html>
<html lang="da">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hjem - GenAI misbrug i IT-sikkerhed</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>

<header>
    <nav>
        <div class="logo">Valgfag - Generative AI Misbrug</div>
        <ul class="nav-links">
            <li><a href="index.html" class="active">Hjem</a></li>
            <li><a href="project.html">Om Projektet</a></li>
            <li><a href="labs.html">Labs</a></li>
            <li><a href="findings.html">Resultater</a></li>
            <li><a href="logbog.html">Logbog</a></li>
            <li><a href="about.html">Om</a></li>
        </ul>
    </nav>
</header>

<div class="container home-container">
    <aside class="sidebar home-sidebar">
        <h2>Navigation</h2>
        <ul>
            <li><a href="#home-summary">Formål</a></li>
            <li><a href="#baggrund">Baggrund og Afgrænsning</a></li>
            <li><a href="#problem-statement">Problemstilling</a></li>
            <li><a href="#key-highlights">Nøglehøjdepunkter</a></li>
            <li><a href="#Leverancer">Leverancer</a></li>
        </ul>
    </aside>

    <main>
    <section id="hero">
        <h1>Velkommen til Valgfag - Generative AI Misbrug i IT-sikkerhed</h1>
        <p class="lead">Udforsk hvordan avanceret AI kan udnyttes til cyberangreb, og hvad kan gøres for at forsvare os.</p>
        <div class="call-to-action">
            <a href="project.html" class="button">Lær mere om Projektet</a>
            <br>
            <a href="labs.html" class="button">Udforsk vores Labs</a>
        </div>
    </section>

    <section id="home-summary">
        <h2>Formål</h2>
        <p>Dette projekt undersøger misbrug af generativ AI inden for cybersikkerhed med fokus på konkrete angrebsteknikker såsom prompt injection, AI-understøttet phishing og social engineering.</p>
        <p>Formålet er at identificere centrale sårbarheder i moderne GenAI-systemer, analysere relevante angrebsvektorer og evaluere tekniske modforanstaltninger med henblik på at styrke organisationers sikkerhed i en AI-drevet trusselsmodel.</p>
    </section>

    <section id="baggrund">
        <h2>Baggrund og Afgrænsning</h2>
        <p>Udviklingen inden for generativ AI har rejst bredere bekymringer blandt forskere som Bengio, Hinton og Hofstadter, særligt omkring ansvarlig anvendelse og ukendte langsigtede konsekvenser. Disse diskussioner hører primært under AI-etik og governance.</p>
        <p>Dette projekt afgrænses imidlertid til den tekniske cybersikkerhedsdimension og fokuserer specifikt på, hvordan generative AI-systemer kan misbruges gennem konkrete angreb såsom prompt injection, social engineering og automatiseret phishing.</p>
        <p>Formålet er ikke at vurdere eksistentielle risici, men at analysere praktiske sikkerhedssårbarheder og demonstrere realistiske misbrugsscenarier, der allerede udgør en trussel mod organisationer i dag.</p>
    </section>


    <section id="problem-statement">
        <h2>Problemstilling</h2>
        <p>Generative AI-systemer introducerer nye angrebsflader, som traditionelle sikkerhedsmodeller ikke er designet til at håndtere. Teknikker såsom prompt injection, modelmisbrug og AI-understøttet social engineering gør det muligt for angribere at omgå sikkerhedskontroller og manipulere output på måder, der kan føre til datalæk, misinformation og kompromittering af brugere.</p>
        <p>Projektets problemstilling er derfor:</p>
        <ul>
            <li><strong>Hvordan kan generative AI-systemer misbruges i praksis?</strong></li>
            <li><strong>Hvilke centrale sårbarheder kan identificeres gennem analyse og eksempler?</strong></li>
            <li><strong>Hvilke tekniske modforanstaltninger kan anvendes for at reducere risikoen for disse angreb?</strong></li>
        </ul>
        <p>Undersøgelsen tager udgangspunkt i OWASP Top 10 for Large Language Models og kombinerer litteraturstudie med illustrative scenarier og udvalgte praktiske demonstrationer.</p>
    </section>


    <section id="key-highlights">
        <h2>Nøglehøjdepunkter</h2>
        <div class="highlight-item">
            <h3>Praktiske Eksperimenter</h3>
            <p>Jeg har udført hands-on eksperimenter med reelle AI-modeller for at simulere og forstå angrebsmekanismerne bag AI-genereret malware, phishing-e-mails og prompt manipulation.</p>
            <p><a href="labs.html">Se Labs for detaljer.</a></p>
        </div>
        <div class="highlight-item">
            <h3>Forskning og Resultater</h3>
            <p>Gennem dybdegående analyse af AI's rolle i social engineering og identitetsmisbrug, præsenterer jeg resultater og diskuterer de bredere implikationer for IT-sikkerhed.</p>
            <p><a href="findings.html">Læs mere om Resultater.</a></p>
        </div>
    </section>

    <section id="Leverancer">
        <h2>Leverancer</h2>
        <ul>
            <li>**Interaktive laboratorier (labs):** Praktiske guides og eksempler på AI-misbrug.</li>
            <li>**Detaljeret Projektdokumentation:** En omfattende rapport og webindhold, der beskriver metoder, analyser og resultater.</li>
            <li>**Videopræsentation:** En kort video, der opsummerer projektet og de vigtigste fund.</li>
        </ul>
    </section>
</main>
</div>

<footer>
    <h3>Referencer</h3>
    <ul>
        <li>Bengio, Y. (2023). <cite>An AI pioneer on the existential risks of his own work</cite>. Interview med The Economist. [Link til kilde]</li>
        <li>Hinton, G. (2023). <cite>Why the 'Godfather of AI' fears for the future</cite>. Interview med CBS News. [Link til kilde]</li>
        <li>Hofstadter, D. (2024). <cite>Prologue "Terrified" i Melanie Mitchell's "Artificial Intelligence: A Guide for Thinking Humans"</cite>. [Bog information]</li>
    </ul>
</footer>

</body>
</html>