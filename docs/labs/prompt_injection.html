<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Prompt Injection Lab</title>
    <link rel="stylesheet" href="../style.css">
</head>
<body>

<header>
    <nav>
        <div class="logo">GenAI Misuse in IT Security</div>
        <ul class="nav-links">
            <li><a href="../index.html">Home</a></li>
            <li><a href="../project.html">Project</a></li>
            <li><a href="../labs.html" class="active">Labs</a></li>
            <li><a href="../findings.html">Findings</a></li>
            <li><a href="../logbog.html">Logbook</a></li>
            <li><a href="../about.html">About</a></li>
        </ul>
    </nav>
</header>

<div class="page">
    <aside class="sidebar">
        <h2>Available Labs</h2>
                <ul>
                </ul>
            </aside>
        
            <main class="content">
                <section>
                    <h1>Prompt Injection Lab</h1>
                    <br>
                    <h2>What is Prompt Injection</h2>
                    <p>Prompt Injection is the act of giving an LLM a malicious payload, either as an act of malice or accidentally, that alters the way the LLM behaves.</p>
                    <p>Prompt Injection could cause LLM's to violate guidelines, generate harmful content, enable unauthorized access or influence critical decisions.</p>
                    
                    <h2>Types of Prompt Injection Vulnerabilities</h2>
                    <h3>Direct Prompt Injections</h3>
                    <p>bla</p>

                    <h3>Indirect Prompt Injections</h3>
                    <p>bla</p>

                    <h2>Prevention and Mitigation</h2>
                    <dl>
                        <dt>1. Constrain model behaviour</dt>
                        <dd>description</dd>
                    </dl>

                    <h2>Example Attacks</h2>
                    <dl>
                        <dt>1. Direct Injection</dt>
                        <dd>text</dd>
                    </dl>

                    
                    <h2>Setup</h2>
                    <ul>
                        <li>Model used: [Specify Model]</li>
                        <li>Environment: [Specify Environment]</li>
                        <li>Tools: [Specify Tools]</li>
                    </ul>
        
                    <h2>Attack Techniques</h2>
                    <p>List the prompts and techniques used.</p>
        
                    <h2>Results</h2>
                    <p>Screenshots and observations.</p>
        
                    <h2>Analysis</h2>
                    <p>Explain why it worked or failed.</p>
        
                    <h2>Countermeasures</h2>
                    <p>What helps against this type of attack.</p>
        
                    <h2>Reflection</h2>
                    <p>What was learned from this lab.</p>
                </section>
            </main>
        </div>
        
        <footer>
            
        </footer>
        
        <script src="../script.js"></script>
        </body>
        </html>