<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LLM07: System Prompt Leakage</title>
    <link rel="stylesheet" href="../style.css">
    <link rel="apple-touch-icon" sizes="180x180" href="../apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="manifest" href="../site.webmanifest">
</head>
<body>

<header>
    <nav>
        <div class="logo">GenAI Misuse in IT Security</div>
        <ul class="nav-links">
            <li><a href="../index.html">Home</a></li>
            <li><a href="../project.html">Project</a></li>
            <li><a href="../research.html" class="active">Research</a></li>
            <li><a href="../findings.html">Findings</a></li>
            <li><a href="../logbog.html">Logbook</a></li>
            <li><a href="../about.html">About</a></li>
        </ul>
    </nav>
</header>

<div class="page">
    <aside class="sidebar">
        <h2>Available Research</h2>
        <ul>
        </ul>
    </aside>
        
    <main class="content">
        <section>
            <h1>LLM07: System Prompt Leakage</h1>

            <section>
                <h2>1. What is System Prompt Leakage</h2>

                <p>
                    System Prompt Leakage is the risk that internal instructions used to steer an LLM behavior may contain sensitive information not intended for users. While the prompt itself is a set of instructions, its disclosure can facilitate other attacks if it contains credentials or internal rules.<sup><a href="https://genai.owasp.org/llmrisk/llm072025-system-prompt-leakage/" target="_blank">[OWASP]</a></sup>.
                </p>

                <p>
                    Disclosure reveals the model internal guardrails, which can then be systematically bypassed by attackers.<sup><a href="https://genai.owasp.org/llmrisk/llm072025-system-prompt-leakage/" target="_blank">[OWASP]</a></sup>.
                </p>

                <p><strong>Potential impacts include:</strong></p>
                <ul>
                    <li>Compromise of system integrity and availability</li>
                    <li>Unauthorized data access or disclosure</li>
                    <li>Financial and reputational damage</li>
                    <li>Safety risks in critical applications</li>
                </ul>
            </section>

            <section>
                <h2>2. Common Examples of Vulnerability</h2>
                <article>
                    <h3>2.1 Exposure of Sensitive Credentials</h3>
                    <p>System prompts that inadvertently include API keys, database connection strings, or internal IDs.</p>
                </article>
                <article>
                    <h3>2.2 Revealing Internal Logic</h3>
                    <p>Prompts disclosing internal decision-making processes, such as loan approval limits or transaction thresholds.</p>
                </article>
                <article>
                    <h3>2.3 Exposure of Filtering Criteria</h3>
                    <p>Revealing the exact keywords or rules the model uses to reject harmful content, allowing attackers to craft bypasses.</p>
                </article>

            </section>

            <section>
                <h2>3. Prevention and Mitigation</h2>

                <p>
                    According to OWASP guidance, risk can be significantly reduced through the following strategies<sup><a href="https://genai.owasp.org/llmrisk/llm072025-system-prompt-leakage/" target="_blank">[OWASP]</a></sup>:
                </p>

                <ul>
                    <li>3.1 Credential Externalization: Never embed API keys or secrets directly in system prompts. Use environment variables or secret managers.</li>
                    <li>3.2 Output Guardrails: Implement external filters to inspect model output for sensitive system information before it reaches the user.</li>
                    <li>3.3 Least Privilege Agents: Use multiple specialized agents with restricted system prompts rather than one large prompt with all permissions.</li>
                    <li>3.4 Avoid Over-Reliance: Do not rely solely on system prompts for security; enforce critical controls independently of the LLM.</li>

                </ul>
            </section>

            <section>
                <h2>4. Example Attack Scenarios</h2>
                <dl>
                    <dt>Scenario #1: Credential Extraction</dt>
                    <dd>An attacker uses a jailbreak prompt to force the model to reveal its system instructions, which include an embedded legacy API key.</dd>
                    <dt>Scenario #2: Guardrail Bypass</dt>
                    <dd>By extracting the system prompt, an attacker learns the specific phrases that trigger the "refusal" response and crafts an injection that avoids them.</dd>
                    <dt>Scenario #3: Rule Manipulation</dt>
                    <dd>An attacker discovers internal banking rules from a chatbot prompt and uses that knowledge to manipulate a transaction request.</dd>

                </dl>
            </section>

            <section>
                <h2>5. References</h2>
                <ul>
                    <li><a href="https://genai.owasp.org/llmrisk/llm072025-system-prompt-leakage/">LLM07: System Prompt Leakage</a></li>
                </ul>
            </section>
        </section>
    </main>
</div>

<footer>
</footer>

<script src="../script.js"></script>
</body>
</html>