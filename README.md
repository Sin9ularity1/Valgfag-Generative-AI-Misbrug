# Mini LLM Chatbot Projektplan (Fra bunden)

Denne projektplan beskriver, hvordan man bygger en **lille transformer-baseret chatbot**
helt fra bunden over ca. 3 måneder med ~10 timer om ugen.

---

## 1. Projektoversigt

**Mål:**  
At bygge en **lille transformer-baseret sprogmodel** (1M–10M parametre), som kan lave basal tekstgenerering og chatlignende svar.

**Problemformulering:**  
Hvordan kan man bygge en lokal AI-chatbot fra bunden, og hvilke tekniske udfordringer opstår undervejs?

**Teknologier:**
- **Python**
- **NumPy** (grundlæggende matematik)
- **PyTorch** (valgfrit, men anbefalet til træning)
- **Ingen eksterne LLM’er**

**Leverancer:**
- Tokenizer
- Datasæt-forberedelse
- Lille Transformer-model
- Træningsscript
- Simpel REPL-chatbot

---

## 2. Metode

Projektet gennemføres iterativt med fokus på små delmål.
Hver komponent udvikles separat og integreres løbende.

Der arbejdes med:

- Eksperimentel udvikling
- Dokumentation undervejs
- Løbende test og evaluering

---

## 3. Milepæle og Tidsplan

### **Uge 1–2 — Tokenizer**

- Implementer en **Byte-Pair Encoding (BPE)** tokenizer fra bunden.
- Trin:
  - Indlæs datasæt.
  - Tæl byte-par frekvenser.
  - Sammenslå top-K par indtil ønsket ordforrådsstørrelse.
  - Opret encode/decode funktioner.

**Output:** `tokenizer.py`, `vocab.json`

---

### **Uge 3–4 — Datasæt og Forbehandling**

- Indsamling af tekstdata (~5–50 MB).
- Rensning og normalisering af tekst.
- Konverter tekst → tokens.
- Opret træningssekvenser.

**Output:** `dataset.py`, `train_data.npy`

---

### **Uge 5–7 — Byg Lille Transformer**

Implementer:

- Token embeddings  
- Positionelle encodings  
- Multi-head self-attention  
- Feed-forward lag  
- LayerNorm  

- Stack **2–6 lag**
- Mål: **1M–10M parametre**

**Output:** `model.py`

---

### **Uge 8–10 — Træningsloop**

- Forward pass  
- Cross entropy loss  
- Backpropagation (PyTorch eller manuelt)  
- Gem checkpoints  
- Simpel evaluering  

**Output:** `train.py`, `model.pt`

---

### **Uge 11 — Inferens Pipeline**

Implementer tekstgenerering:

- Sampling  
- Top-k  
- Temperature  

**Output:** `generate.py`

---

### **Uge 12 — Chat Interface**

- Byg en simpel terminal-chatbot.
- Integrér tokenizer + model + inferens.

**Output:** `chat.py`

---

## 4. Begrænsninger

Projektet er begrænset af hardware, tid og modelstørrelse.
Der fokuseres på læring og forståelse frem for maksimal performance.

--- 

## 5. Ekstra Mål (Valgfrit)

- Tilføj LoRA fine-tuning  
- Tilføj GUI  
- Tilføj hukommelsesbuffer  
- Destillér modellen  
- Tilføj emotionelle/stil-indstillinger  

---

## 6. Forventet Læringsudbytte

- Forstå tokenisering på et dybt niveau  
- Lære transformer-arkitektur  
- Bygge et neuralt netværk fra bunden  
- Forstå træningsdynamikker  
- Implementere en minimal men reel sprogmodel  

---

## 7. Læringsmål

### AI & Machine Learning
- Forklare hvordan tokenisering fungerer (BPE) og selv implementere en simpel tokenizer.
- Redegøre for transformer-arkitekturens hoveddele (embeddings, attention, feed-forward, layer normalization).
- Forklare hvordan en sprogmodel trænes (loss, backpropagation, epochs).
- Implementere en lille transformer-model fra bunden.
- Forklare forskellen mellem træning og inferens.
---
### Programmering
- Udvikle et større Python-projekt struktureret i moduler.
- Arbejde med NumPy-arrays og tensors.
- Bruge PyTorch til at bygge og træne neurale netværk.
- Implementere egne træningsloops.
- Arbejde med filformater (json, npy, pt).
---
### IT-sikkerhed & ansvarlig AI
- Reflektere over datasæt, bias og modelbegrænsninger.
- Forstå sikkerhedsrisici ved lokale AI-modeller.
- Diskutere etiske aspekter ved AI-chatbots.
- Forklare hvordan simple AI-systemer kan misbruges eller manipuleres.
--- 
### Projektarbejde 
- Planlægge et teknisk projekt med milepæle
- Dokumentere kode og proces løbende
- Arbejde iterativt med fejlretning og forbedringer
- Bruge GitHub til versionskontrol
